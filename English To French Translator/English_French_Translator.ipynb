{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2pukCDIb7yT"
   },
   "source": [
    "# <center><u> <h1>English French-Translator</h1></u></center>\n",
    "\n",
    "Savez-vous comment créer une application de traduction de langue?<br>\n",
    "Did you understand the above sentence?<br>\n",
    "Well after googling it, I found its meaning as:<br>\n",
    "Do you know how to create a language translator app?<br>\n",
    "\n",
    "We all know about Google Translate which allows us to convert from one language to another and it’s very useful for learning and understanding new languages.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "![](https://daleonai.com/images/2019-11-05-improving-machine-translation-with-the-google-translation-api-advanced/1.png)\n",
    "\n",
    "\n",
    "In this project I aiming to convert English phrases to French using RNN on Deep Learning Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yva_P1dHb6aH"
   },
   "source": [
    "## Introduction\n",
    "In this notebook, I built a deep neural network that functions as part of an end-to-end machine translation pipeline. The completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "Preprocess - I converted text to sequence of integers.\n",
    "Models Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations.\n",
    "Prediction Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4115,
     "status": "ok",
     "timestamp": 1711968816210,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "1wmao4O0cnLS"
   },
   "outputs": [],
   "source": [
    "# Now import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1711968828319,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "Y_tcQPP0eozk",
    "outputId": "dcd1dcf9-c21b-49fc-e770-d74cdf6e85dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Importing Google Drive for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aziw3H_Wcyll"
   },
   "source": [
    "## Load Data\n",
    "The data is located in data/small_vocab_en and data/small_vocab_fr. The small_vocab_en file contains English sentences with their French translations in the small_vocab_fr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1711968844085,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "cboaEhaZcvm3"
   },
   "outputs": [],
   "source": [
    "# Define the paths for English and French data\n",
    "english_data = \"/content/drive/MyDrive/Deep Learning Projects/English To French Translator/small_vocab_en.txt\"\n",
    "french_data = \"/content/drive/MyDrive/Deep Learning Projects/English To French Translator/small_vocab_fr.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnOhjwXI1VYO"
   },
   "source": [
    "- The OS module in Python provides functions for interacting with the operating system\n",
    "- The code loads the data from a file called input_file. The code then splits the string of text into an array using split(). <br>\n",
    "Then, it uses list comprehension to create a list with each line in the array as its own element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1711968892090,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "7ozqkItv1Pd6"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "import os\n",
    "\n",
    "#define a function with one parameter path\n",
    "def load_data(path):\n",
    "  #join the path with join keyword\n",
    "  input_file = os.path.join(path)\n",
    "  #open file and read  as f\n",
    "  with open (input_file,\"r\") as f:\n",
    "    #read file\n",
    "    data = f.read()\n",
    "  #return with data split(\"\\n\")\n",
    "  return data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiwjeZzVCMo8"
   },
   "source": [
    "Now loading all english and french data into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1711968953072,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "crFQWCx2eLsv"
   },
   "outputs": [],
   "source": [
    "# Load English and French data\n",
    "english_sentences = load_data(english_data)\n",
    "french_sentences = load_data(french_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw60fkIXgCB4"
   },
   "source": [
    "## Analysis of Dataset\n",
    "Let's look at few examples in the dataset of both language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1711968992033,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "9V_5yFcKgI-4",
    "outputId": "ac30679d-5c23-4ebc-897c-1f574382b1fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:  0\n",
      "English:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "--------------------------------------------------\n",
      "Sample:  1\n",
      "English:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "--------------------------------------------------\n",
      "Sample:  2\n",
      "English:  california is usually quiet during march , and it is usually hot in june .\n",
      "French:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "--------------------------------------------------\n",
      "Sample:  3\n",
      "English:  the united states is sometimes mild during june , and it is cold in september .\n",
      "French:  les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "--------------------------------------------------\n",
      "Sample:  4\n",
      "English:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "French:  votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print sample sentences from both languages\n",
    "for i in range(5):\n",
    "    print(\"Sample: \", i)\n",
    "    print(\"English: \", english_sentences[i])\n",
    "    print(\"French: \", french_sentences[i])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EV6ri_Shkff"
   },
   "source": [
    "## Convert to Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1711969017541,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "_3fQhJ5J2E4D"
   },
   "outputs": [],
   "source": [
    "# Import collections for counting words\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 999,
     "status": "ok",
     "timestamp": 1711969049232,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "_U3gqv5WhOTy",
    "outputId": "bb822115-de36-4c7c-c0aa-61a13367f37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 227\n",
      "French Vocabulary Size: 355\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words in English and French\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "print(\"English Vocabulary Size:\", len(english_words_counter))\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "print(\"French Vocabulary Size:\", len(french_words_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsl03eu-jW09"
   },
   "source": [
    "## Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be numbers.\n",
    "\n",
    "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's Tokenizer function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1711969079677,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "cYLtZ0WmjTjD"
   },
   "outputs": [],
   "source": [
    "# Define a function to tokenize text\n",
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKiftBNLIttH"
   },
   "source": [
    "- The code starts by tokenizing the text_sentences list into individual sentences.Then, it prints out the word index of each sentence in the text_tokenized list.<br>\n",
    "- Next, it iterates through each sentence and prints out a sample output for that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1711969163277,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "lkhE2V5bjyg_",
    "outputId": "70e48e20-0434-42b5-8cf0-e867a86f2ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index:\n",
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sample text\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .'\n",
    "]\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(\"Word Index:\")\n",
    "print(text_tokenizer.word_index)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfiSCxQ7kRK3"
   },
   "source": [
    "## Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, adding padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Making sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1711969206398,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "6Daf_ngukNIS"
   },
   "outputs": [],
   "source": [
    "# Define a function to pad sequences\n",
    "def pad(x, length=None):\n",
    "    return pad_sequences(x, maxlen=length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LMUzdBGKei0"
   },
   "source": [
    "- The code is used to preprocess the input data set. - The tokenize function splits the text into individual tokens, which are then passed to a function called pad that takes in a list of tokens and pads them with a specified character (in this case, spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1711969254554,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "9LHAL30ik5x4"
   },
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10900,
     "status": "ok",
     "timestamp": 1711969284642,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "fSefb358UclR"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1711969313358,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "glQKhXvpUjDx",
    "outputId": "18113681-84fb-4365-e927-68543284c446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "# Print preprocessing information\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print(\"Data Preprocessed\")\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n7i4rD_22_J"
   },
   "source": [
    "## Create Model\n",
    "\n",
    "I will begin by training four relatively simple architectures:\n",
    "- 1. Model 1 is a simple RNN\n",
    "- 2. Model 2 is a RNN with Embedding\n",
    "- 3. Model 3 is a Bidirectional RNN\n",
    "- 4. Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, I will construct a deeper architecture that is designed to outperform all four models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSLZv-O_3Bgq"
   },
   "source": [
    "## Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want. We want the French translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1711969362707,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "7BDBJKw22yJa"
   },
   "outputs": [],
   "source": [
    "# Define a function to convert logits to text\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return \" \".join([index_to_words[prediction] for prediction in np.argmax(logits, axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN-RjzvH3rpp"
   },
   "source": [
    "## Building Model\n",
    "Here I used RNN model combined with GRU nodes for translation\n",
    "\n",
    "- The code starts by defining the input shape and output sequence length. Next, it defines the number of unique English words in the dataset and French words in the dataset. The code then builds a Keras model using word embedding on x and y. It also sets hyperparameters for learning rate, which is 0.005, as well as building layers for this model. Finally, it compiles this model with sparse_categorical_crossentropy loss function and Adam optimizer with learning rate set to 0.005. The code will create a Keras model that has been trained to recognize words in English and French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1711969432455,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "ta5yGpZ23amq"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Build and train a RNN model using word embedding on x and y\n",
    "  :param input_shape: Tuple of input shape\n",
    "  :param output_sequence_length: Length of output sequence\n",
    "  :param english_vocab_size: Number of unique English words in the dataset\n",
    "  :param french_vocab_size: Number of unique French words in the dataset\n",
    "  :return: Keras model built, but not trained\n",
    "\"\"\"\n",
    "# Define the embedding model\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 0.005\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1]))\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation=\"relu\")))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation=\"softmax\")))\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1711969458907,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "ubkWXBpu32Jh"
   },
   "outputs": [],
   "source": [
    "# Reshape the input for the model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NXsCzht3-1y"
   },
   "source": [
    "Finally calling the model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144589,
     "status": "ok",
     "timestamp": 1711969659166,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "8B9QZgl738qJ",
    "outputId": "fbc94d7c-a360-4135-82f9-88b66f70f5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 14s 72ms/step - loss: 1.3953 - accuracy: 0.6746 - val_loss: 0.4998 - val_accuracy: 0.8409\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.4203 - accuracy: 0.8627 - val_loss: 0.3087 - val_accuracy: 0.8953\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.2992 - accuracy: 0.9003 - val_loss: 0.2427 - val_accuracy: 0.9191\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.2488 - accuracy: 0.9166 - val_loss: 0.2206 - val_accuracy: 0.9249\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.2247 - accuracy: 0.9236 - val_loss: 0.2088 - val_accuracy: 0.9284\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.2073 - accuracy: 0.9286 - val_loss: 0.1991 - val_accuracy: 0.9306\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1996 - accuracy: 0.9307 - val_loss: 0.1937 - val_accuracy: 0.9330\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1923 - accuracy: 0.9329 - val_loss: 0.1881 - val_accuracy: 0.9342\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1838 - accuracy: 0.9347 - val_loss: 0.1819 - val_accuracy: 0.9360\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1797 - accuracy: 0.9357 - val_loss: 0.1819 - val_accuracy: 0.9363\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1787 - accuracy: 0.9359 - val_loss: 0.1822 - val_accuracy: 0.9364\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1753 - accuracy: 0.9371 - val_loss: 0.1883 - val_accuracy: 0.9343\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1763 - accuracy: 0.9366 - val_loss: 0.1883 - val_accuracy: 0.9351\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1728 - accuracy: 0.9377 - val_loss: 0.1846 - val_accuracy: 0.9371\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 0.1691 - accuracy: 0.9387 - val_loss: 0.1812 - val_accuracy: 0.9376\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1660 - accuracy: 0.9395 - val_loss: 0.1779 - val_accuracy: 0.9387\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1661 - accuracy: 0.9395 - val_loss: 0.1784 - val_accuracy: 0.9384\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1699 - accuracy: 0.9385 - val_loss: 0.1805 - val_accuracy: 0.9375\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.1671 - accuracy: 0.9393 - val_loss: 0.1782 - val_accuracy: 0.9386\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 0.1637 - accuracy: 0.9401 - val_loss: 0.1794 - val_accuracy: 0.9387\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "simple_rnn_model = embed_model(tmp_x.shape, preproc_french_sentences.shape[1], len(english_tokenizer.word_index) + 1, len(french_tokenizer.word_index) + 1)\n",
    "history = simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "301fqre_WAuU"
   },
   "source": [
    "Printing model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1711969659167,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "R-2dQpBQ4EZa",
    "outputId": "796d17cc-7630-4cfd-d170-df92fc937e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 21, 1024)          263168    \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 21, 345)           353625    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1062745 (4.05 MB)\n",
      "Trainable params: 1062745 (4.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#print Model summary\n",
    "simple_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HodMPRh1WNys"
   },
   "source": [
    "## Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1711969680836,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "Um2UZ6EbSge9",
    "outputId": "238e0586-9077-422b-d1b4-ba2f48f79f08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "simple_rnn_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg0KtRlXBHPE"
   },
   "source": [
    "## Arbitrary Predictions\n",
    "Performing predictions on the models using User Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1711969796830,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "ys8_8FhABNfx"
   },
   "outputs": [],
   "source": [
    "# Define the final predictions function\n",
    "def final_predictions(text):\n",
    "    y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_french_sentences.shape[-2], padding='post')\n",
    "    translated_text = logits_to_text(simple_rnn_model.predict(sentence[:1])[0], french_tokenizer)\n",
    "    translated_text = \" \".join([word for word in translated_text.split() if word != '<PAD>'])\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 7513,
     "status": "ok",
     "timestamp": 1711969807745,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "3Hvqno_9IBIC",
    "outputId": "c789d46c-b935-46ce-9bcb-a1880b69c7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's easy\n",
      "1/1 [==============================] - 1s 544ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'il est facile'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNJp0UldE2g6"
   },
   "source": [
    "#Implementation\n",
    "Enter your input here to get predictions. We will using Gradio for implementation part. Refer video for detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6915,
     "status": "ok",
     "timestamp": 1711969833174,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "ydqiUysprgoA",
    "outputId": "f0280c6f-9bcf-488b-c446-95ed680e87d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.110.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.4)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (2023.6.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.36.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Gradio\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tURKFZaa6Gy"
   },
   "source": [
    "After installing, we import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPNaXvU_bu3O"
   },
   "source": [
    "- The code creates an interface with a function called final_predictions. - The inputs of the interface are a textbox that has two lines and a placeholder, which is \"Text to translate\". - The outputs of the interface are \"text\". - The code launches the program in debug mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2620,
     "status": "ok",
     "timestamp": 1711969856045,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "DKrXgzLOWstV"
   },
   "outputs": [],
   "source": [
    "# Import Gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "executionInfo": {
     "elapsed": 5792,
     "status": "ok",
     "timestamp": 1711969926583,
     "user": {
      "displayName": "Akash Pasi",
      "userId": "00195711684523598234"
     },
     "user_tz": -330
    },
    "id": "0eAJ2TuFpKKU",
    "outputId": "2b80f86b-4d5c-474d-f231-c9a47724c526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://3ad4ba87b65abdf3e9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3ad4ba87b65abdf3e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Gradio interface\n",
    "def translate_text(text):\n",
    "    translated_text = final_predictions(text)\n",
    "    return translated_text\n",
    "\n",
    "gr.Interface(fn=translate_text, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1TTniKZT8gkghDmEJZPiJoXAE6AR2MuX2",
     "timestamp": 1643821766311
    },
    {
     "file_id": "https://github.com/projjal1/English-French-Translator-RNN/blob/master/English_French_Translator.ipynb",
     "timestamp": 1643375239879
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
