{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5CFBuyEbZyz"
   },
   "source": [
    "<h2 align=\"center\">BERT_Email_Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvuwji11oeVT"
   },
   "source": [
    "In this tutorial, I have built a spam detection model. The spam detection model will classify emails as spam or not spam. This will be used to filter unwanted and unsolicited emails. I have built this model using BERT and Tensorflow.\n",
    "\n",
    "BERT will be used to generate sentence encoding for all emails. Finally, I have used Tensorflow to build the neural networks. Tensorflow will create the input and output layers of our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBw3YgISrOOu"
   },
   "source": [
    "## Importing important packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxbdNalNq5Sp"
   },
   "source": [
    "tensorflow_text: It will allow us to work with text. In this tutorial, we are solving a text-classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j1Y_C7hShgbY"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-text==2.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Osw2qO1ijwkv",
    "outputId": "c282018c-d71d-4dc8-d2ba-9e67e44fee3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-4.25.0-py3-none-any.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting gradio-client==0.15.0 (from gradio)\n",
      "  Downloading gradio_client-0.15.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.0->gradio) (2023.6.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.15.0->gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=22b5e6a83e5b81ec84d309963d87fd14ab36ee95a6e827538e53ca0a0b754bb8\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
      "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.1 ffmpy-0.3.2 gradio-4.25.0 gradio-client-0.15.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.5 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWQzFioArk63"
   },
   "source": [
    "TensorFlow: It is the machine learning package used to build the neural network. It creates the input and output layers of my machine learning model.\n",
    "\n",
    "TensorFlow Hub: It contains a pre-trained machine model used to build our text classification. Our pre-trained model is BERT. I will re-use the BERT model and fine-tune it to meet my needs.\n",
    "\n",
    "TensorFlow Text: It allows us to work with text.\n",
    "\n",
    "Pandas: We will use Pandas to load our dataset. I will also use Pandas for data manipulation and analysis. It gives me a clear overview of how my dataset is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "12VGb57gbZzA"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VV7kUz6gbZzE"
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(\"/content/spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJRpCDaYFRXj",
    "outputId": "55427ce5-c00f-4962-dfa2-6012e887495d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Display top 5 rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOGLn2sFsVNH"
   },
   "source": [
    "The dataset has two categories: ham and spam. \"Ham\" represents emails that are not spam; these are emails from a trusted source. \"Spam\" represents emails from an unknown source.\n",
    "\n",
    "The dataset also includes the Message column, which represents the email messages. Let's examine the individual value count for the spam and ham emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4aZRX7dbZzI",
    "outputId": "f804bd1a-188c-4e4f-cac0-ebda53d6858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      " ham     4825\n",
      "spam     747\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the count of categories\n",
    "category_counts = df['Category'].value_counts()\n",
    "print(\"Category Counts:\\n\", category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXLEsyoaukH_"
   },
   "source": [
    "The dataset has 4,825 ham emails and 747 spam emails. The number of ham emails is significantly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzd3v8SBbZzK",
    "outputId": "f507ffa5-1e37-471d-c9a5-2b85487e238f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Ratio: 13.41%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the ratio of spam to ham\n",
    "spam_ratio = (category_counts['spam'] / (category_counts['spam'] + category_counts['ham'])) * 100\n",
    "print(\"Spam Ratio: {:.2f}%\".format(spam_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13QTAqukyNCD"
   },
   "source": [
    "This result implies that about 13% of the emails are spam, while 87% are ham emails. This indicates a class imbalance, and I need to balance the two classes to reduce bias during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdXVBG5lyeoC"
   },
   "source": [
    "## Balancing dataset\n",
    "There are various techniques used to balance the dataset. I will employ the simplest approach by reducing the majority class from 4825 to 747, thereby achieving a balanced distribution between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgFI1jg3bZzN",
    "outputId": "154d7a1a-90e8-4b36-851e-91a7528bddef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the shape of spam\n",
    "df_spam = df[df['Category'] == 'spam']\n",
    "#print shape\n",
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6z-7-kbxbZzP",
    "outputId": "5e4eedc6-bd0b-411b-fd24-f8be8f40a560"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4825, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the shape of ham\n",
    "df_ham = df[df['Category'] == 'ham']\n",
    "#print shape\n",
    "df_ham.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPxPq9zu0KnK"
   },
   "source": [
    "Now that I have created the two data frames, I will reduce the number of instances in the ham class to match that of the spam class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M13oNuQ4bZzQ",
    "outputId": "66bed829-63bc-4c4e-a872-40fd9f7736b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample the ham messages to balance the dataset\n",
    "df_ham_downsampled = df_ham.sample(df_spam.shape[0])\n",
    "\n",
    "#print the shape\n",
    "df_ham_downsampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__buvWm73ZH7"
   },
   "source": [
    "I will save the new class into a variable called df_ham_downsampled. I need to concatenate the two balanced classes into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bF6rbrfbZzR",
    "outputId": "ea4c417f-9e7a-42ab-dad3-e7a5b28a115c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the downsampled ham and spam messages\n",
    "df_balanced = pd.concat([df_ham_downsampled, df_spam])\n",
    "\n",
    "#print the shape\n",
    "df_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7T9m7nx39Fe"
   },
   "source": [
    "The pd.concat method will concatenate df_ham_downsampled and df_spam into a single DataFrame and save the dataset into a variable called df_balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj_KpqPrbZzS",
    "outputId": "f6fb7dd5-d477-4ddf-e563-c667224739c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Category Counts:\n",
      " ham     747\n",
      "spam    747\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the balanced count of categories\n",
    "balanced_category_counts = df_balanced['Category'].value_counts()\n",
    "print(\"Balanced Category Counts:\\n\", balanced_category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0nJ6oLa4EtH"
   },
   "source": [
    "## Adding labels\n",
    "I need to label our dataset as 1 and 0. '1' will represent the data samples belonging to the spam class, while '0' will represent those belonging to the ham class.\n",
    "\n",
    "I will use lambda to write the logic, and then the apply method will execute this logic, enabling us to label the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PO2yULZCbZzU",
    "outputId": "3deac695-b452-4aaa-bce4-762c10dae845",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_balanced\",\n  \"rows\": 1494,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1377,\n        \"samples\": [\n          \"Ta-Daaaaa! I am home babe, are you still up ?\",\n          \"Here got ur favorite oyster... N got my favorite sashimi... Ok lar i dun say already... Wait ur stomach start rumbling...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spam\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df_balanced"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9086e617-a150-47a9-9d64-537a741cd125\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yo, any way we could pick something up tonight?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>ham</td>\n",
       "      <td>No my blankets are sufficient, thx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>ham</td>\n",
       "      <td>As usual..iam fine, happy &amp;amp; doing well..:)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sun ah... Thk mayb can if dun have anythin on....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>ham</td>\n",
       "      <td>Derp. Which is worse, a dude who always wants ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9086e617-a150-47a9-9d64-537a741cd125')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9086e617-a150-47a9-9d64-537a741cd125 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9086e617-a150-47a9-9d64-537a741cd125');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-05b52f6a-f0db-4991-a064-f0804fa123b9\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05b52f6a-f0db-4991-a064-f0804fa123b9')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-05b52f6a-f0db-4991-a064-f0804fa123b9 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     Category                                            Message  spam\n",
       "2922      ham    Yo, any way we could pick something up tonight?     0\n",
       "1037      ham                 No my blankets are sufficient, thx     0\n",
       "5257      ham     As usual..iam fine, happy &amp; doing well..:)     0\n",
       "4554      ham  Sun ah... Thk mayb can if dun have anythin on....     0\n",
       "2754      ham  Derp. Which is worse, a dude who always wants ...     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary label for spam (1) and ham (0)\n",
    "df_balanced['spam'] = df_balanced['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0JZpGK74oPv"
   },
   "source": [
    "The dataset is labeled into two categories: some data samples are labeled as 1, while others are labeled as 0. Now, I need to split the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKbrtnbRbZzV"
   },
   "source": [
    "## Split it into training and test dataset\n",
    "\n",
    "I split the dataset into two sets: the first set will be used for training, and the second set will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gnTw4-MmbZzW"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced[\"Message\"], df_balanced['spam'], stratify=df_balanced['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCAoAUsabZzY",
    "outputId": "0affea80-cb46-428e-c5f6-a41fdc15d14a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635     Dear Voucher Holder, 2 claim this weeks offer,...\n",
       "27      Did you catch the bus ? Are you frying an egg ...\n",
       "2562                              And maybe some pressies\n",
       "2899          If you r @ home then come down within 5 min\n",
       "4436    Don't b floppy... b snappy & happy! Only gay c...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the head of X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C1OTIC_5Qck"
   },
   "source": [
    "we use stratify to ensure equal distribution of classes in the train and test sample. This ensures we have an equal amount of spam and ham emails after splitting. After splitting the dataset, we can start working with BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVzS6I4xbZzb"
   },
   "source": [
    "## Getting started with BERT\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers. BERT models help machines understand and interpret the meaning of text by using immediately preceding text to grasp the context and checking the relationships of words within a sentence to determine their actual meaning.\n",
    "\n",
    "BERT converts a given sentence into an embedding vector, which represents the unique words in a document. This ensures that words with similar meanings have similar representations.\n",
    "\n",
    "Since machine learning operates effectively with numbers rather than text, BERT converts input text into embedding vectors, facilitating model processing.\n",
    "\n",
    "The BERT process comprises two stages: Preprocessing and Encoding.\n",
    "\n",
    "Preprocessing is the initial stage in BERT where noise is removed from the dataset, duplicates are eliminated, and the dataset is formatted for ease of use during model training, thereby enhancing model performance.\n",
    "\n",
    "Encoding, the subsequent stage, involves converting text into real numbers, which is crucial since machine learning algorithms work more effectively with numerical data. BERT accomplishes this by converting sentences into embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJCT24gQ5kNQ"
   },
   "source": [
    "## Downloading the BERT model\n",
    "BERT models are typically pre-trained and available in TensorFlow Hub, which contains all the pre-trained machine learning models that can be downloaded.\n",
    "\n",
    "I will download two models: one for performing preprocessing and the other for encoding. The links for the models are provided below.\n",
    "\n",
    "for bert_preprocess:<br>\n",
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "\n",
    "for bert_encoder:<br>\n",
    "\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "spe5N6zGbZzb"
   },
   "outputs": [],
   "source": [
    "#download the pre-trained BERT models with hub.kerasLayer\n",
    "bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArXBq6g35rCA"
   },
   "source": [
    "Building model using TensorFlow\n",
    "There are two types of models that one can build in TensorFlow: the Sequential model and the Functional model. In a Sequential model, layers are built on top of each other, one layer at a time. However, in a Sequential model, it does not have multiple inputs and outputs.\n",
    "\n",
    "On the other hand, Functional models are more robust and flexible. They do not necessarily create layers in a strictly sequential order. Instead, in the Functional model, there can be multiple inputs and outputs. I will use the Functional approach to build the model, starting by initializing the BERT layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lM2aHqAbZzj"
   },
   "source": [
    "<h4>Build Model</h4>\n",
    "\n",
    "The input layer is created using the tf.keras.layers.Input method. I will use the preprocessed_text as input for this layer.\n",
    "\n",
    "The bert_encoder function will then convert the preprocessed text into embedding vectors. These vectors will serve as the output of this layer. The outputs will then be fed into the neural network layers.\n",
    "\n",
    "The neural network comprises two layers: the Dropout layer and the Dense layer.\n",
    "\n",
    "Dropout Layer:\n",
    "This layer will be used to prevent model overfitting. I will set the dropout rate to 0.1% to address overfitting, which occurs when a model excessively learns from training data but performs poorly during testing.\n",
    "\n",
    "Since I am using the functional approach to build the model, I will define the input for this layer as a function using (outputs['pooled_output']). This input corresponds to the output of the BERT layers.\n",
    "\n",
    "Dense Layer:\n",
    "This layer contains only one neuron. I will initialize the activation function as sigmoid. Sigmoid is suitable when the output values need to be between 0 and 1. In this case, during predictions, the probability of prediction will range from 0 to 1, making sigmoid the most appropriate choice.\n",
    "\n",
    "The model will take text_input as inputs and will produce only one output. I will display the model summary to visualize all the input and output layers used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_FpwKjFObZzk"
   },
   "outputs": [],
   "source": [
    "# Define input and output layers for BERT\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_input')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Add neural network layers\n",
    "dropout = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(dropout)\n",
    "\n",
    "# Construct the model\n",
    "model = tf.keras.Model(inputs=text_input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RZEQeD4CnXB"
   },
   "source": [
    "Printing the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EgeOrb8bZzo",
    "outputId": "9c92a818-1493-4e0b-9ddf-e9e6d38974c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_word_ids':   0           ['text_input[0][0]']             \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'default': (None,   109482241   ['keras_layer[0][0]',            \n",
      "                                768),                             'keras_layer[0][1]',            \n",
      "                                 'pooled_output': (               'keras_layer[0][2]']            \n",
      "                                None, 768),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768)}                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 769\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#print the summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEnE5r1-1sP3"
   },
   "source": [
    "I have initialized all the input and output layers for our model. The output also displays the total params, trainable params, and non-trainable params.\n",
    "\n",
    "Total params: This represents all the parameters in the model.\n",
    "\n",
    "Trainable params: These represent the parameters that I will train.\n",
    "\n",
    "Non-trainable params: These parameters are from the BERT model, and they are already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFpmjWpDbZzq",
    "outputId": "f02c8825-5af3-4d4e-d4b7-3b397244eca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the len\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJNedv-H1ynC"
   },
   "source": [
    "The optimizer is used to improve model performance and reduce errors that occur during model training. I use the Adam optimizer.\n",
    "\n",
    "Metrics will be used to check the model's performance so that I can assess how well we trained our model. I set the BinaryAccuracy(name='accuracy') metric, which will be used to calculate the accuracy score of the model.\n",
    "\n",
    "The loss function is used to calculate the model error during the training phase. I use binary_crossentropy as my loss function because the output is binary; it can either be a 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "omqD2jJqbZzq"
   },
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt60X_TgbZzr"
   },
   "source": [
    "<h4>Train the model</h4>\n",
    "\n",
    "The model learns from the training data samples, identifying patterns within the dataset to gain knowledge.\n",
    "\n",
    "I will specify the number of epochs as 10. The model will iterate through the dataset ten times and print the accuracy score after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9N9a3I9GbZzs",
    "outputId": "91ccafa5-6594-4fa8-c71a-5ba847463898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - 526s 15s/step - loss: 0.6408 - accuracy: 0.6375 - precision: 0.6341 - recall: 0.6500\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 522s 15s/step - loss: 0.5072 - accuracy: 0.8143 - precision: 0.7933 - recall: 0.8500\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 522s 15s/step - loss: 0.4370 - accuracy: 0.8571 - precision: 0.8509 - recall: 0.8661\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 521s 15s/step - loss: 0.3831 - accuracy: 0.8839 - precision: 0.8682 - recall: 0.9054\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 519s 15s/step - loss: 0.3526 - accuracy: 0.8982 - precision: 0.8858 - recall: 0.9143\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 521s 15s/step - loss: 0.3338 - accuracy: 0.8875 - precision: 0.8848 - recall: 0.8911\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 514s 15s/step - loss: 0.3165 - accuracy: 0.8866 - precision: 0.8676 - recall: 0.9125\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 506s 14s/step - loss: 0.3022 - accuracy: 0.9054 - precision: 0.9054 - recall: 0.9054\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 521s 15s/step - loss: 0.2873 - accuracy: 0.9080 - precision: 0.9059 - recall: 0.9107\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 521s 15s/step - loss: 0.2808 - accuracy: 0.9098 - precision: 0.8991 - recall: 0.9232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff2f8d0130>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVW09nNTbZzs",
    "outputId": "a0ca2708-6fc9-4d5d-844a-31b3ceb8cbfd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 175s 14s/step - loss: 0.2719 - accuracy: 0.9091 - precision: 0.9091 - recall: 0.9091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2719302177429199,\n",
       " 0.9090909361839294,\n",
       " 0.9090909361839294,\n",
       " 0.9090909361839294]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1Je0os12Jlh"
   },
   "source": [
    "## Evaluating model using the testing dataset\n",
    "\n",
    "The model.predict method will yield the prediction results in a 2D array, yet I require my results in a 1D array. To achieve this conversion from 2D to 1D array, I utilize the y_predicted.flatten() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIExcaN9bZzt",
    "outputId": "8d3d7d60-59fb-465f-f1d0-6eab7ad72eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels:\n",
      " [0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
      " 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
      " 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
      " 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0\n",
      " 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#predict our model\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = y_predicted.flatten()\n",
    "y_predicted = np.where(y_predicted > 0.5, 1, 0)\n",
    "print(\"Predicted Labels:\\n\", y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGqDhbvOGES1"
   },
   "source": [
    "Since I used a sigmoid activation function, the prediction probabilities will lie between 0.0 and 1.0. Therefore, if the prediction result is > 0.5, the output should be 1, and if it is < 0.5, the output should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKK5LwVIbZzv",
    "outputId": "9d80abed-aa9f-4cb6-e2d8-296b2cfb07a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[170  17]\n",
      " [ 17 170]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsdjcE4pbZzw",
    "outputId": "851100b4-fab7-41fd-d9b8-5a852f9231e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       187\n",
      "           1       0.91      0.91      0.91       187\n",
      "\n",
      "    accuracy                           0.91       374\n",
      "   macro avg       0.91      0.91      0.91       374\n",
      "weighted avg       0.91      0.91      0.91       374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlyLFvRMbZzw"
   },
   "source": [
    "## Try your inputs\n",
    "\n",
    "You can change your inputs as per you choice:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AU8YXWUObZzx",
    "outputId": "312c167a-4ea1-4859-b051-64f7503d1755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels for Sample Reviews:\n",
      " [[0.66333693]\n",
      " [0.7872124 ]\n",
      " [0.71563506]\n",
      " [0.1036278 ]\n",
      " [0.06791556]]\n",
      "Predicted Labels for Sample Reviews:\n",
      " ['spam', 'spam', 'spam', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Sample reviews for prediction\n",
    "reviews = [\n",
    "    'Enter a chance to win $5000, hurry up, offer valid until march 31, 2021',\n",
    "    'You are awarded a SiPix Digital Camera! call 09061221061 from landline. Delivery within 28days. T Cs Box177. M221BP. 2yr warranty. 150ppm. 16 . p pÂ£3.99',\n",
    "    'it to 80488. Your 500 free text messages are valid until 31 December 2005.',\n",
    "    'Hey Sam, Are you coming for a cricket game tomorrow',\n",
    "    \"Why don't you wait 'til at least wednesday to see if you get your.\"\n",
    "]\n",
    "\n",
    "# Predict labels for sample reviews\n",
    "predicted_reviews = model.predict(reviews)\n",
    "print(\"Predicted Labels for Sample Reviews:\\n\", predicted_reviews)\n",
    "\n",
    "# Convert predicted labels to 'ham' or 'spam'\n",
    "predicted_labels = [\"ham\" if pred < 0.5 else \"spam\" for pred in predicted_reviews.flatten()]\n",
    "print(\"Predicted Labels for Sample Reviews:\\n\",predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZLUh7BlmPuW"
   },
   "source": [
    "From the output above, the first three email messages have been classified as spam, as they have a prediction probability greater than 0.5. The last two email messages have been classified as ham, with a prediction probability less than 0.5. These are the correct predictions and demonstrate that we have successfully built our text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "raUtnwGnkCL_",
    "outputId": "9f787089-7de3-47a9-e8cb-9761913c76ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://50cbcac7ca0f1c7c04.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://50cbcac7ca0f1c7c04.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to predict labels for input text\n",
    "def predict_spam_or_ham(text):\n",
    "    # Predict labels for input text\n",
    "    predicted_reviews = model.predict([text])\n",
    "    # Convert predicted labels to 'ham' or 'spam'\n",
    "    predicted_label = \"ham\" if predicted_reviews.flatten()[0] < 0.5 else \"spam\"\n",
    "    return predicted_label\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(fn=predict_spam_or_ham, inputs=\"text\", outputs=\"text\", title=\"Spam or Ham Mail Detector\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qvd0UcomSqd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSffNIx4mUL2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
